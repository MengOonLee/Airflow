{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd4f0dc-8c09-4c9d-a299-aa19600fecc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87164d8a-eea7-41a3-b126-da51d1f8800e",
   "metadata": {},
   "source": [
    "### DAG example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e15fe-1723-4fd6-a6b8-7c56447c16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from airflow import DAG\n",
    "\n",
    "reporting_dag = DAG(\n",
    "    dag_id=\"publish_EMEA_sales_report\",\n",
    "    # Insert the cron expression\n",
    "    schedule_interval=\"0 7 * * 1\",\n",
    "    start_date=datetime.datetime(2019, 11, 24),\n",
    "    default_args={\n",
    "        \"owner\": \"sales\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3312ad-3ad8-4141-a622-538dcbc22fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from airflow.models import DAG\n",
    "\n",
    "default_arguments = {\n",
    "    'owner': 'jdoe',\n",
    "    'email': 'jdoe@datacamp.com',\n",
    "    'start_date': datetime.datetime(2020, 1, 20)\n",
    "}\n",
    "\n",
    "etl_dag = DAG(\n",
    "    dag_id='etl_pipeline',\n",
    "    schedule_interval=\"* * * * *\",\n",
    "    default_args=default_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d911e7-4bc7-4c67-b79d-e4f266e2bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "dag = DAG(dag_id='example_dag',\n",
    "    default_args={\"start_date\": \"2021-07-31\"}\n",
    ")\n",
    "task01 = BashOperator(task_id='generate_random_number',\n",
    "    bash_command='echo $RANDOM',\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08843ca1-6a4d-4ff9-9558-2d91f3741b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# descriptions\n",
    "airflow -h\n",
    "\n",
    "# Run specific task\n",
    "airflow run <dag_id> <task_id> <start_date>\n",
    "\n",
    "# test specific task\n",
    "airflow test <dag_id> <task_id> -1\n",
    "\n",
    "# Run full DAG\n",
    "airflow trigger_dag -e <date> <dag_id>\n",
    "\n",
    "# start webserver on port\n",
    "airflow webserver -p <port>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fa852-a950-4579-a55e-0244d8eb312b",
   "metadata": {},
   "source": [
    "## Airflow DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e6bc9-9a50-43ce-824d-c80fb6d4983b",
   "metadata": {},
   "source": [
    "### Airflow operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73ea7d-37ce-4712-9972-df88a85adfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BashOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "example_task = BashOperator(\n",
    "    task_id='bash_example',\n",
    "    bash_command='cat addresses.txt | awk \"NF==10\" > cleaned.txt',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "bash_task = BashOperator(\n",
    "    task_id='bash_script_example',\n",
    "    bash_command='cleanup.sh',\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3ca50-0391-4933-ae66-e098dae830bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PythonOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "    \n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable.\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments.\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', \n",
    "        'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba2f27-4393-4a05-a340-db76045ef646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PythonOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from my_library import my_magic_function\n",
    "\n",
    "python_task = PythonOperator(\n",
    "    dag=dag,\n",
    "    task_id='perform_magic',\n",
    "    python_callable=my_magic_function,\n",
    "    op_kwargs={\n",
    "        \"snowflake\": '*\",\n",
    "        \"amount\": 42\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f0a68-0d6f-49c7-b7ba-3442aad074e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmailOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "email_task = EmailOperator(\n",
    "    task_id='email_sales_report',\n",
    "    to='sales_manager@example.com',\n",
    "    subject='Automated Sales Report',\n",
    "    html_content='Attached is the latest sales report',\n",
    "    files='latest_sales.xlsx',\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4033e27-0aee-406c-b73a-edf467758d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSHOperator\n",
    "from airflow.contrib.operators.ssh_operator import SSHOperator\n",
    "\n",
    "spark_master = (\n",
    "    \"spark://\"\n",
    "    \"spark_standalone_cluster_ip\"\n",
    "    \":7077\"\n",
    ")\n",
    "command = (\n",
    "    \"spark-submit \"\n",
    "    \"--master {master} \"\n",
    "    \"--py-files package1.zip \"\n",
    "    \"/path/to/app.py\"\n",
    ").format(master=spark_master)\n",
    "\n",
    "task = SSHOperator(\n",
    "    task_id='ssh_spark_submit',\n",
    "    dag=dag,\n",
    "    command=command,\n",
    "    ssh_conn_id='spark_master_ssh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf9f24-49e7-4c6f-8e13-356b12209a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SparkSubmitOperator\n",
    "from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator\n",
    "\n",
    "spark_task = SparkSubmitOperator(\n",
    "    task_id='spark_submit_id',\n",
    "    dag=dag,\n",
    "    applicaton=\"/path/to/app.py\",\n",
    "    py_files=\"package1.zip\",\n",
    "    conn_id='spark_default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e6a68-7cac-405a-bc86-8aef3e446969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from airflow import DAG\n",
    "\n",
    "# Create a DAG object\n",
    "dag = DAG(\n",
    "    dag_id=\"optimize_diaper_purchases\",\n",
    "    # Run the DAG daily\n",
    "    schedule_interval='@daily',\n",
    "    # Specify when tasks should have started earliest\n",
    "    start_date=datetime.datetime(2019, 6, 25),\n",
    "    default_args={\n",
    "        # Don't email on failure\n",
    "        'email_on_failure': False\n",
    "    }\n",
    ")\n",
    "\n",
    "config = os.path.join(os.environ[\"AIRFLOW_HOME\"], \n",
    "    \"scripts\", \"configs\", \"data_lake.conf\")\n",
    "\n",
    "ingest = BashOperator(\n",
    "  # Assign a descriptive id\n",
    "  task_id=\"ingest_data\", \n",
    "  # Complete the ingestion pipeline\n",
    "  bash_command=\"tap-marketing-api | target-csv --config %s\" % config,\n",
    "  dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b60619-99e8-4fcd-a625-900ebd3bb9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "echo_file = \"\"\"\n",
    "    {% for filename in params.filenames %}\n",
    "        echo \"Reading {{filename}}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "with DAG(\n",
    "    'Tutorial03',\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2021, 8, 1),\n",
    "    schedule_interval='0 13 * * *'\n",
    ") as dag:\n",
    "# Define the tasks    \n",
    "    task01 = BashOperator(\n",
    "        task_id='template_task',\n",
    "        bash_command=echo_file,\n",
    "        params={'filenames': ['file01.txt', 'file02.txt']}\n",
    "    )\n",
    "    \n",
    "    task01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373bee9e-2cb6-4786-a810-8f7e6da98e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"squad-a\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2019, 7, 5),\n",
    "    \"email\": [\"foo@bar.com\"],\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    \"cleaning\",\n",
    "    default_args=default_args,\n",
    "    user_defined_macros={\n",
    "        \"env\": Variable.get(\"environment\")\n",
    "    },\n",
    "    schedule_interval=\"0 5 */2 * *\"\n",
    ")\n",
    "\n",
    "def say(what):\n",
    "    print(what)\n",
    "\n",
    "with dag:\n",
    "    say_hello = BashOperator(\n",
    "        task_id=\"say-hello\", \n",
    "        bash_command=\"echo Hello,\"\n",
    "    )\n",
    "    say_world = BashOperator(\n",
    "        task_id=\"say-world\", \n",
    "        bash_command=\"echo World\"\n",
    "    )\n",
    "    shout = PythonOperator(\n",
    "        task_id=\"shout\",\n",
    "        python_callable=say,\n",
    "        op_kwargs={'what': '!'}\n",
    "    )\n",
    "\n",
    "    say_hello >> say_world >> shout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fd79e-db0f-4d7b-9726-70a03c664ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "def print_func():\n",
    "    print(\"This goes in the logs!\")\n",
    "    \n",
    "def sleep_func(length_of_time):\n",
    "    sleep(length_of_time)\n",
    "\n",
    "with DAG(\n",
    "    'Tutorial02',\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2021, 8, 1),\n",
    "    schedule_interval='0 0 * * *'\n",
    ") as dag:\n",
    "# Define the tasks    \n",
    "    task01 = PythonOperator(\n",
    "        task_id='print',\n",
    "        python_callable=print_func\n",
    "    )\n",
    "    task02 = PythonOperator(\n",
    "        task_id='sleep',\n",
    "        python_callable=sleep_func,\n",
    "        op_kwargs={'length_of_time': 5}\n",
    "    )\n",
    "    \n",
    "    task01 >> task02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f100a-b414-4d92-b45d-d40c303441e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the operator\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator\n",
    "\n",
    "# Set the path for our files.\n",
    "entry_point = os.path.join(\n",
    "    os.environ[\"AIRFLOW_HOME\"], \"scripts\", \"clean_ratings.py\")\n",
    "dependency_path = os.path.join(\n",
    "    os.environ[\"AIRFLOW_HOME\"], \"dependencies\", \"pydiaper.zip\")\n",
    "\n",
    "with DAG(\n",
    "    dag_id='data_pipeline', \n",
    "    start_date=datetime(2019, 6, 25),\n",
    "    schedule_interval='@daily'\n",
    "    ) as dag:\n",
    "    # Define task clean, running a cleaning job.\n",
    "    clean_data = SparkSubmitOperator(\n",
    "        application=entry_point, \n",
    "        py_files=dependency_path,\n",
    "        task_id='clean_data',\n",
    "        conn_id='spark_default'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abb74b-e984-4cae-aec8-d151600bae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_args = {\"py_files\": dependency_path,\n",
    "              \"conn_id\": \"spark_default\"}\n",
    "# Define ingest, clean and transform job.\n",
    "with dag:\n",
    "    ingest = BashOperator(\n",
    "        task_id='Ingest_data', \n",
    "        bash_command='tap-marketing-api | target-csv --config %s' % config\n",
    "    )\n",
    "    clean = SparkSubmitOperator(\n",
    "        application=clean_path, \n",
    "        task_id='clean_data', \n",
    "        **spark_args\n",
    "    )\n",
    "    insight = SparkSubmitOperator(\n",
    "        application=transform_path, \n",
    "        task_id='show_report', \n",
    "        **spark_args\n",
    "    )\n",
    "    \n",
    "    # set triggering sequence\n",
    "    ingest >> clean >> insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaecef8-37d0-4335-ad06-5a5f7a6822b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow.models import DAG\n",
    "from airflow.providers.google.cloud.operators.bigquery import \\\n",
    "    BigQueryCreateEmptyDatasetOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "default_args = {\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'BigQuery_test',\n",
    "    default_args=default_args,\n",
    "    start_date=days_ago(1),\n",
    "    schedule_interval='0 0 * * *',\n",
    "    tags=['Sushi_King']\n",
    ") as dag:\n",
    "    \n",
    "    task_create_dataset = BigQueryCreateEmptyDatasetOperator(\n",
    "        task_id='create-dataset',\n",
    "        gcp_conn_id='google_cloud_default',\n",
    "        dataset_id='test_dataset',\n",
    "        location='asia-southeast1'\n",
    "    )\n",
    "    \n",
    "    task_create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1f260-9373-43e3-90a7-28f58e36c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "tz=pytz.timezone(\"Asia/Kuala_Lumpur\")\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.now(tz=tz) - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"tutorial\",\n",
    "    start_date=yesterday,\n",
    "    schedule_interval=\"@daily\"\n",
    "    ) as dag:\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id=\"echo\",\n",
    "        bash_command=\"echo $PWD\"\n",
    "    )\n",
    "    t2 = EmailOperator(\n",
    "        task_id='send_email',\n",
    "        conn_id='sendgrid_default',\n",
    "        to='darklemon2000@gmail.com',\n",
    "        subject=\"EmailOperator test for SendGrid\",\n",
    "        html_content=\"This is a test message sent through SendGrid.\"\n",
    "    )\n",
    "    t1 >> t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073eb4c-9d02-4e46-9df2-24537f7f28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from textwrap import dedent\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"Mydin_scrapy\",\n",
    "    default_args={\n",
    "        \"email\": [\"darklemonlee@yahoo.co.uk\"],\n",
    "        \"email_on_failure\": True\n",
    "    },\n",
    "    start_date=yesterday,\n",
    "    schedule_interval=\"@daily\",\n",
    "    tags=[\"Test\"]\n",
    ") as dag:\n",
    "    \n",
    "    grocery = BashOperator(\n",
    "        task_id=\"grocery\",\n",
    "        bash_command=dedent(\"\"\"\n",
    "            cd ${AIRFLOW_HOME}/dags/Scrapy/Mydin\n",
    "            scrapy crawl grocery -O {{ params.filename }}_{{ ds_nodash }}.jl\n",
    "        \"\"\"),\n",
    "        params={\n",
    "            \"filename\": \"./data/grocery\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49406692-2443-466f-995f-68584c192f71",
   "metadata": {},
   "source": [
    "### Airflow tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e573826-c697-4c50-95aa-ce75841b87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "task1 = BashOperator(\n",
    "    task_id='first_task',\n",
    "    bash_command='echo 1',\n",
    "    dag=dag\n",
    ")\n",
    "task2 = BashOperator(\n",
    "    task_id='second_task',\n",
    "    bash_command='echo 2',\n",
    "    dag=dag\n",
    ")\n",
    "# Set first_task to run before second_task\n",
    "task1 >> task2 # task1.set_downstream(task2)\n",
    "# or\n",
    "task2 << task1 # task2.set_upstream(task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873eed8-3c2f-4afb-a5a0-fe472f75e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "with DAG(\n",
    "    'Tutorial01',\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2021, 8, 1),\n",
    "    schedule_interval='0 12 * * *'\n",
    ") as dag:\n",
    "# Define the tasks    \n",
    "    task01 = BashOperator(\n",
    "        task_id='1stecho',\n",
    "        bash_command='echo $PWD'\n",
    "    )\n",
    "    task02 = BashOperator(\n",
    "        task_id='run-script',\n",
    "        bash_command='Tutorial01.sh'\n",
    "    )\n",
    "    task03 = BashOperator(\n",
    "        task_id='2ndecho',\n",
    "        bash_command='ls && echo'\n",
    "    )\n",
    "    # Set task01 to run before task02\n",
    "    task01 >> task02\n",
    "    # Set task03 to run before task02\n",
    "    task02 << task03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668486a1-9143-453d-8d27-ecacfecfd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'parallel_dag',\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2021, 8, 1),\n",
    "    schedule_interval='@daily'\n",
    ") as dag:\n",
    "    \n",
    "    task_1 = BashOperator(\n",
    "        task_id='task_1',\n",
    "        bash_command='sleep 3'\n",
    "    )\n",
    "    \n",
    "    task_2 = BashOperator(\n",
    "        task_id='task_2',\n",
    "        bash_command='sleep 3'\n",
    "    )\n",
    "    \n",
    "    task_3 = BashOperator(\n",
    "        task_id='task_3',\n",
    "        bash_command='sleep 3'\n",
    "    )\n",
    "    \n",
    "    task_4 = BashOperator(\n",
    "        task_id='task_4',\n",
    "        bash_command='sleep 3'\n",
    "    )\n",
    "    \n",
    "    task_1 >> [task_2, task_3] >> task_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bc906-6d6e-485d-be41-40fe7d2e9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return 'even_day_task'\n",
    "    else:\n",
    "        return 'odd_day_task'\n",
    "\n",
    "with DAG(\n",
    "    'Tutorial04',\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2021, 7, 30),\n",
    "    schedule_interval='@daily'\n",
    ") as dag:\n",
    "    \n",
    "    # Define the tasks\n",
    "    start_task = BashOperator(\n",
    "        task_id='start',\n",
    "        bash_command='echo START'\n",
    "    )\n",
    "    branch_task = BranchPythonOperator(\n",
    "        task_id='branch',\n",
    "        provide_context=True,\n",
    "        python_callable=branch_test\n",
    "    )\n",
    "    even_day_task = BashOperator(\n",
    "        task_id='even_day_task',\n",
    "        bash_command='echo EVEN day'\n",
    "    )\n",
    "    odd_day_task = BashOperator(\n",
    "        task_id='odd_day_task',\n",
    "        bash_command='echo ODD day'\n",
    "    )\n",
    "    \n",
    "    start_task >> branch_task >> even_day_task\n",
    "    branch_task >> odd_day_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdea04-24bf-48ec-9f2d-75181e8fdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "import sys\n",
    "\n",
    "dag = DAG(\n",
    "   dag_id = 'update_state',\n",
    "   default_args={\"start_date\": \"2019-10-01\"}\n",
    ")\n",
    "task1 = BashOperator(\n",
    "   task_id='generate_random_number',\n",
    "   bash_command='echo $RANDOM',\n",
    "   dag=dag\n",
    ")\n",
    "\n",
    "def python_version():\n",
    "    return sys.version\n",
    "\n",
    "task2 = PythonOperator(\n",
    "   task_id='get_python_version',\n",
    "   python_callable=python_version,\n",
    "   dag=dag\n",
    ")   \n",
    "task3 = SimpleHttpOperator(\n",
    "   task_id='query_server_for_external_ip',\n",
    "   endpoint='https://api.ipify.org',\n",
    "   method='GET',\n",
    "   dag=dag\n",
    ")   \n",
    "task3 >> [task2, task1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea18ea6-7f20-4787-9a9f-2437e23faba9",
   "metadata": {},
   "source": [
    "### Airflow scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910d779-5555-41f6-9c96-09ebfeb2171d",
   "metadata": {},
   "source": [
    "Schedule the task at `start_date` + `schedule_interval`  \n",
    "'start_date': datetime(2020, 2, 25)  \n",
    "'schedule_interval': @daily\n",
    "\n",
    "- `start_date`: The date / time to initially schedule the DAG run\n",
    "- `end_date`: Optional attribute for when to stop running new DAG instances\n",
    "- `max_tries`: Optional attribute for how many attempts to make\n",
    "- `schedule_interval`: How often to schedule via `cron` style syntax or via built-in presets.\n",
    "\n",
    "**Cron syntax**  \n",
    "`* * * * *`: minute(0-59), hour(0-23), dayofmonth(1-31), month(1-12), dayofweek(0-6)  \n",
    "`0,15,30,45 * * * *`: Run every 15 minutes  \n",
    "`*/15 9-17 * * 1-3,5` Run every 15 minutes on 9am-5pm every Mon, Tue, Wed & Fri\n",
    "\n",
    "**Scheduler presets**  \n",
    "- `@once`\n",
    "- `@hourly`\n",
    "- `@daily`\n",
    "- `@weekly`\n",
    "- `@monthly`\n",
    "- `@yearly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dad7c2-940c-426c-af71-3aa795655916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "dag = DAG('update_dataflows', \n",
    "    default_args=default_args, \n",
    "    schedule_interval='30 12 * * 3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c3d0e-3a69-496f-a021-71556df5bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner':'sales_eng',\n",
    "    'start_date': datetime(2020, 2, 15),\n",
    "}\n",
    "process_sales_dag = DAG(\n",
    "    dag_id='process_sales', \n",
    "    default_args=default_args, \n",
    "    schedule_interval='@monthly'\n",
    ")\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'w') as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "def parse_file(inputfile, outputfile):\n",
    "    with open(inputfile) as infile:\n",
    "      data=json.load(infile)\n",
    "      with open(outputfile, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38255d-a1a7-494a-9101-e21ed599176e",
   "metadata": {},
   "source": [
    "## Airflow workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb25171-dced-4b24-9c0d-1987cf6a3bae",
   "metadata": {},
   "source": [
    "### Airflow sensors\n",
    "\n",
    "`airflow.sensors.base_sensor_operator` waits for a certain condition to be true\n",
    "\n",
    "- `mode='poke'`: run repeatedly  \n",
    "- `mode='reschedule'`: give up task and try again later\n",
    "- `poke_interval`: wait time between checks\n",
    "- `timeout`: wait time before failing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d291d8-8f12-4343-ad58-dbe046052bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "file_sensor_task = FileSensor(\n",
    "    task_id='file_sense',\n",
    "    filepath='salesdata.csv',\n",
    "    poke_interval=300,\n",
    "    dag=sales_report_dag\n",
    ")\n",
    "\n",
    "init_sales_cleanup >> file_sensor_task >> generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfeae6b-74ab-4d68-abf8-c33cb38b933d",
   "metadata": {},
   "source": [
    "### Airflow executors\n",
    "\n",
    "- `SequentialExecutor` runs one task at a time  \n",
    "- `LocalExecutor` runs simultaneous tasks  \n",
    "- `CeleryExecutor` runs extensive workflows  \n",
    "\n",
    "`!cat airflow/airflow.cfg | grep \"executor = \"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6330315-5517-42f0-8ca9-1bfd3b598cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Debugging\n",
    "airflow list_dags\n",
    "\n",
    "# fix scheduler\n",
    "airflow scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e9064-ab84-4e5b-915d-99d4b4106665",
   "metadata": {},
   "source": [
    "### SLAs & reporting in Airflow\n",
    "\n",
    "Service Level Agreements (SLAs) = amount of time a DAG/task should require to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41f220-adb0-4930-a288-e42dfd8d4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Create the dictionary entry\n",
    "defaut_args = {\n",
    "    'start_date': datetime(2020, 2, 20),\n",
    "    'sla': timedelta(minutes=20)\n",
    "}\n",
    "# Add to the DAG\n",
    "dag = DAG('sla_dag', \n",
    "    default_args=default_args,\n",
    "    schedule_interval='@None'\n",
    ")\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(\n",
    "    task_id='sla_task',\n",
    "    bash_command='runcode.sh',\n",
    "    sla=timedelta(seconds=30),\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9643a-011d-4800-b208-bf025cae3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2020, 2, 15)\n",
    "}\n",
    "report_dag = DAG(\n",
    "    dag_id='execute_report', \n",
    "    default_args=default_args, \n",
    "    schedule_interval='@monthly'\n",
    ")\n",
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "    task_id='email_report',\n",
    "    to='airflow@gmail.com',\n",
    "    subject='Airflow Monthly Report',\n",
    "    html_content=\"\"\"\n",
    "    Attached is your mounthly worflow report - please refer to it for more detail\n",
    "    \"\"\",\n",
    "    files=['monthly_report.pdf'],\n",
    "    dag=report_dag\n",
    ")\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a77875-e0d5-4251-95e2-dd97eb138253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com', 'airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success': True,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "report_dag = DAG(\n",
    "    dag_id='execute_report',\n",
    "    schedule_interval=\"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag\n",
    ")\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7ebd6-23d2-427b-9f0d-2353a387257e",
   "metadata": {},
   "source": [
    "## Production pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e279a-c04d-41a2-80c8-19d5d06eb5ac",
   "metadata": {},
   "source": [
    "### Templates\n",
    "\n",
    "- substitute information during DAG run  \n",
    "- `Jinja` templating language\n",
    "\n",
    "### Runtime variables\n",
    "\n",
    "`! help(<Airflow object>)` & look for `template_fields`\n",
    "\n",
    "1. Execution date YYYY-MM-DD: {{ ds }}\n",
    "2. Execution date, no dashes YYYYMMDD: {{ ds_nodash }}\n",
    "3. Previous execution date YYYY-MM-DD: {{ prev_ds }}\n",
    "4. Previous execution date, no dashes YYYYMMDD: {{ prev_ds_nodash }}\n",
    "5. DAG object: {{ dag }}\n",
    "6. Airflow config object: {{ conf }}\n",
    "\n",
    "### Macros\n",
    "\n",
    "- `{{ macros.datetime }}`: datetime.datetime object\n",
    "- `{{ macros.timedelta }}`: timedelta object\n",
    "- `{{ macros.uuid }}`: uuid object\n",
    "- `{{ macros.ds_add('2020-04-15', 5) }}`: 2020-04-20\n",
    "\n",
    "### Branching\n",
    "\n",
    "- conditional logic\n",
    "- `airflow.operators.python_operator.BranchPythonOperator`\n",
    "- `python_callable` returns next task id to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee035b19-8587-435b-9ee1-cd9e21b5764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "cleandata_dag = DAG('cleandata',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily'\n",
    ")\n",
    "\n",
    "# Create a templated command to execute\n",
    "# 'bash cleandata.sh datestring filename'\n",
    "templated_command = \"\"\"\n",
    "bash cleandata.sh {{ ds_nodash }} {{ params.filename }}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(\n",
    "    task_id='cleandata_task',\n",
    "    bash_command=templated_command,\n",
    "    params={'filename': 'salesdata.txt'},\n",
    "    dag=cleandata_dag\n",
    ")\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(\n",
    "    task_id='cleandata_task2',\n",
    "    bash_command=templated_command,\n",
    "    params={'filename': 'supportdata.txt'},\n",
    "    dag=cleandata_dag\n",
    ")\n",
    "# Set the operator dependencies\n",
    "clean_task >> clean_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c5eb8-dfef-4a4e-919f-b7e54fa95e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily'\n",
    ")\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(\n",
    "    task_id='cleandata_task',\n",
    "    bash_command=templated_command,\n",
    "    params={'filenames': filelist},\n",
    "    dag=cleandata_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d360655-5e4b-4c67-b033-2ba0eb88a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "    default_args={'start_date': datetime(2020, 4, 15)},\n",
    "    schedule_interval='@weekly'\n",
    ")\n",
    "                \n",
    "email_task = EmailOperator(\n",
    "    task_id='email_task',\n",
    "    to='testuser@datacamp.com',\n",
    "    subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "    html_content=html_email_str,\n",
    "    params={'username': 'testemailuser'},\n",
    "    dag=email_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb0cf3-6b95-49f4-ac98-4305661cb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return 'even_day_task'\n",
    "    else:\n",
    "        return 'odd_day_task'\n",
    "\n",
    "dag = DAG('BranchingTest', \n",
    "    default_args={'start_date': datetime(2020, 4, 15)}, \n",
    "    schedule_interval='@daily'\n",
    ")\n",
    "\n",
    "start_task = DummyOperator(\n",
    "    task_id='start_task', \n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "branch_task = BranchPythonOperator(\n",
    "    task_id='branch_task',\n",
    "    provide_context=True,\n",
    "    python_callable=branch_test,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "even_day_task = DummyOperator(\n",
    "    task_id='even_day_task', \n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "even_day_task2 = DummyOperator(\n",
    "    task_id='even_day_task2', \n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "odd_day_task = DummyOperator(\n",
    "    task_id='odd_day_task', \n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "odd_day_task2 = DummyOperator(\n",
    "    task_id='odd_day_task2', \n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "start_task >> branch_task >> even_day_task >> even_day_task2\n",
    "branch_task >> odd_day_task >> odd_day_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba77c1-e5ad-4026-8a5a-5b517641fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "\n",
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(\n",
    "    task_id='branch_task', \n",
    "    dag=branch_dag,\n",
    "    python_callable=year_check, \n",
    "    provide_context=True\n",
    ")\n",
    "# Define the dependencies\n",
    "branch_dag >> current_year_task\n",
    "branch_dag >> new_year_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd393ab-c457-490d-852f-d6370cc3cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/process.py\n",
    "from datetime import date\n",
    "\n",
    "def process_data(**kwargs):\n",
    "    file = open(\"/home/repl/workspace/processed_data-\" + kwargs['ds'] + \".tmp\", \"w\")\n",
    "    file.write(f\"Data processed on {date.today()}\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b620fb9-b24c-44a4-bac1-dc17cb20acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/pipeline.py\n",
    "# Import the needed operators\n",
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2019, 1, 1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(\n",
    "    dag_id='etl_update', \n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "sensor = FileSensor(\n",
    "    task_id='sense_file', \n",
    "    filepath='/home/repl/workspace/startprocess.txt',\n",
    "    poke_interval=5,\n",
    "    timeout=15,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "bash_task = BashOperator(\n",
    "    task_id='cleanup_tempfiles', \n",
    "    bash_command='rm -f /home/repl/*.tmp',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "python_task = PythonOperator(\n",
    "    task_id='run_processing', \n",
    "    python_callable=process_data,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "email_subject = \"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "email_report_task = EmailOperator(\n",
    "    task_id='email_report_task',\n",
    "    to='sales@mycompany.com',\n",
    "    subject=email_subject,\n",
    "    html_content='',\n",
    "    params={'department': 'Data subscription services'},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "no_email_task = DummyOperator(\n",
    "    task_id='no_email_task',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'], \"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat /Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(\n",
    "    task_id='check_if_weekend',\n",
    "    python_callable=check_weekend,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a958678-b522-41bb-8ccd-66b87c028221",
   "metadata": {},
   "source": [
    "## Deployment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f0acf-430d-4e3d-b8bb-b9565cc77957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DagBag\n",
    "\n",
    "def test_dagbag_import():\n",
    "    \"\"\"\n",
    "    Verify that Airflow will be able to import all DAGs in the repository.\n",
    "    \"\"\"\n",
    "    dagbag = DagBag()\n",
    "    number_of_failures = len(dagbag.import_errors)\n",
    "    assert number_of_failures == 0, \\\n",
    "        \"There should be no DAG failures. Got: %s\" dagbag.import_errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
