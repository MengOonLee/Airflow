{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c6ce7a-cea1-437f-bd3c-52e3425a8cef",
   "metadata": {},
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d104616-1f67-4bb1-a27e-dadada814ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/meng/work/Airflow/Build/Docker/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/work/Airflow/Build/Docker/requirements.txt\n",
    "numpy\n",
    "pandas\n",
    "plotly\n",
    "dash\n",
    "scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4bad04-5498-4168-b93d-c51c543ad847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/meng/work/Airflow/Build/Docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/work/Airflow/Build/Docker/Dockerfile\n",
    "FROM apache/airflow:slim-latest\n",
    "\n",
    "USER root\n",
    "\n",
    "RUN apt-get update -y \\\n",
    "    && apt-get install -y --no-install-recommends \\\n",
    "        python3-dev \\\n",
    "        python3-venv \\\n",
    "    && apt-get upgrade -y \\\n",
    "    && apt-get autoremove -y --purge \\\n",
    "    && apt-get clean -y \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY --chown=airflow:root ~/work/Airflow/dags /opt/airflow/dags\n",
    "\n",
    "USER airflow\n",
    "\n",
    "ENV VENV=/home/airflow/venv\n",
    "\n",
    "COPY requirements.txt .\n",
    "\n",
    "RUN python3 -m venv $VENV \\\n",
    "    && python3 -m pip install --no-cache-dir -U pip wheel \\\n",
    "    && pip install --no-cache-dir -Ur requirements.txt \\\n",
    "    && pip check \\\n",
    "    && rm -rf requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3f637-2bf2-4836-9c82-3598998e1858",
   "metadata": {},
   "source": [
    "### Docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a69499-a2e7-49c2-b775-26567a3980c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ~/work/Airflow/Build/Docker\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1995bae-607a-48f3-be9c-575505a25b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/meng/work/Airflow/docker-compose.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/work/Airflow/docker-compose.yaml\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "#\n",
    "\n",
    "# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\n",
    "#\n",
    "# WARNING: This configuration is for local development. Do not use it in a production deployment.\n",
    "#\n",
    "# This configuration supports basic configuration using environment variables or an .env file\n",
    "# The following variables are supported:\n",
    "#\n",
    "# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\n",
    "#                                Default: apache/airflow:2.5.0\n",
    "# AIRFLOW_UID                  - User ID in Airflow containers\n",
    "#                                Default: 50000\n",
    "# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\n",
    "#\n",
    "# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\n",
    "#                                Default: airflow\n",
    "# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\n",
    "#                                Default: airflow\n",
    "# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\n",
    "#                                Default: ''\n",
    "#\n",
    "# Feel free to modify this file to suit your needs.\n",
    "---\n",
    "version: '3'\n",
    "x-airflow-common:\n",
    "  &airflow-common\n",
    "  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n",
    "  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n",
    "  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n",
    "  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.5.0}\n",
    "  # build: .\n",
    "  environment:\n",
    "    &airflow-common-env\n",
    "    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n",
    "    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n",
    "    # For backward compatibility, with Airflow <2.3\n",
    "    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n",
    "    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n",
    "    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n",
    "    AIRFLOW__CORE__FERNET_KEY: ''\n",
    "    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n",
    "    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n",
    "    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'\n",
    "    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n",
    "  volumes:\n",
    "    - ./dags:/opt/airflow/dags\n",
    "    - ./logs:/opt/airflow/logs\n",
    "    - ./plugins:/opt/airflow/plugins\n",
    "  user: \"${AIRFLOW_UID:-50000}:0\"\n",
    "  depends_on:\n",
    "    &airflow-common-depends-on\n",
    "    redis:\n",
    "      condition: service_healthy\n",
    "    postgres:\n",
    "      condition: service_healthy\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      POSTGRES_USER: airflow\n",
    "      POSTGRES_PASSWORD: airflow\n",
    "      POSTGRES_DB: airflow\n",
    "    volumes:\n",
    "      - postgres-db-volume:/var/lib/postgresql/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n",
    "      interval: 5s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "\n",
    "  redis:\n",
    "    image: redis:latest\n",
    "    expose:\n",
    "      - 6379\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
    "      interval: 5s\n",
    "      timeout: 30s\n",
    "      retries: 50\n",
    "    restart: always\n",
    "\n",
    "  airflow-webserver:\n",
    "    <<: *airflow-common\n",
    "    command: webserver\n",
    "    ports:\n",
    "      - 8080:8080\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-scheduler:\n",
    "    <<: *airflow-common\n",
    "    command: scheduler\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", 'airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\"']\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-worker:\n",
    "    <<: *airflow-common\n",
    "    command: celery worker\n",
    "    healthcheck:\n",
    "      test:\n",
    "        - \"CMD-SHELL\"\n",
    "        - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      # Required to handle warm shutdown of the celery workers properly\n",
    "      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n",
    "      DUMB_INIT_SETSID: \"0\"\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-triggerer:\n",
    "    <<: *airflow-common\n",
    "    command: triggerer\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", 'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"']\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "  airflow-init:\n",
    "    <<: *airflow-common\n",
    "    entrypoint: /bin/bash\n",
    "    # yamllint disable rule:line-length\n",
    "    command:\n",
    "      - -c\n",
    "      - |\n",
    "        function ver() {\n",
    "          printf \"%04d%04d%04d%04d\" $${1//./ }\n",
    "        }\n",
    "        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && gosu airflow airflow version)\n",
    "        airflow_version_comparable=$$(ver $${airflow_version})\n",
    "        min_airflow_version=2.2.0\n",
    "        min_airflow_version_comparable=$$(ver $${min_airflow_version})\n",
    "        if (( airflow_version_comparable < min_airflow_version_comparable )); then\n",
    "          echo\n",
    "          echo -e \"\\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\\e[0m\"\n",
    "          echo \"The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!\"\n",
    "          echo\n",
    "          exit 1\n",
    "        fi\n",
    "        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n",
    "          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n",
    "          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n",
    "          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n",
    "          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n",
    "          echo\n",
    "        fi\n",
    "        one_meg=1048576\n",
    "        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n",
    "        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n",
    "        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n",
    "        warning_resources=\"false\"\n",
    "        if (( mem_available < 4000 )) ; then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n",
    "          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n",
    "          echo\n",
    "          warning_resources=\"true\"\n",
    "        fi\n",
    "        if (( cpus_available < 2 )); then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n",
    "          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n",
    "          echo\n",
    "          warning_resources=\"true\"\n",
    "        fi\n",
    "        if (( disk_available < one_meg * 10 )); then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n",
    "          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n",
    "          echo\n",
    "          warning_resources=\"true\"\n",
    "        fi\n",
    "        if [[ $${warning_resources} == \"true\" ]]; then\n",
    "          echo\n",
    "          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n",
    "          echo \"Please follow the instructions to increase amount of resources available:\"\n",
    "          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n",
    "          echo\n",
    "        fi\n",
    "        mkdir -p /sources/logs /sources/dags /sources/plugins\n",
    "        chown -R \"${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}\n",
    "        exec /entrypoint airflow version\n",
    "    # yamllint enable rule:line-length\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      _AIRFLOW_DB_UPGRADE: 'true'\n",
    "      _AIRFLOW_WWW_USER_CREATE: 'true'\n",
    "      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n",
    "      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n",
    "      _PIP_ADDITIONAL_REQUIREMENTS: ''\n",
    "    user: \"0:0\"\n",
    "    volumes:\n",
    "      - .:/sources\n",
    "\n",
    "  airflow-cli:\n",
    "    <<: *airflow-common\n",
    "    profiles:\n",
    "      - debug\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      CONNECTION_CHECK_MAX_COUNT: \"0\"\n",
    "    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n",
    "    command:\n",
    "      - bash\n",
    "      - -c\n",
    "      - airflow\n",
    "\n",
    "  # You can enable flower by adding \"--profile flower\" option e.g. docker-compose --profile flower up\n",
    "  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n",
    "  # See: https://docs.docker.com/compose/profiles/\n",
    "  flower:\n",
    "    <<: *airflow-common\n",
    "    command: celery flower\n",
    "    profiles:\n",
    "      - flower\n",
    "    ports:\n",
    "      - 5555:5555\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "    depends_on:\n",
    "      <<: *airflow-common-depends-on\n",
    "      airflow-init:\n",
    "        condition: service_completed_successfully\n",
    "\n",
    "volumes:\n",
    "  postgres-db-volume:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa89eae-3d50-4e2e-995d-d0ba6b7a1d6b",
   "metadata": {},
   "source": [
    "### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da1afb2-4ddd-4223-b005-5a897d33cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/meng/work/Airflow/build.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/work/Airflow/build.sh\n",
    "#!/bin/bash\n",
    "\n",
    "docker build . --no-cache --pull \\\n",
    "-f ~/Work/Airflow/Build/Docker/Dockerfile \\\n",
    "-t darklemon/airflow:latest\n",
    "\n",
    "docker push darklemon/airflow:latest\n",
    "\n",
    "echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n",
    "\n",
    "echo -e \"AIRFLOW_IMAGE_NAME=darklemon/airflow:latest\" >> .env\n",
    "\n",
    "docker compose up airflow-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77dd6f1-cdad-4f5f-8661-ee6f393c0720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/meng/work/Airflow/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/meng/work/Airflow/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "docker compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afe0aad-6b4c-47d8-87ca-9cff831b73c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/meng/work/Airflow/uninstall.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/meng/work/Airflow/uninstall.sh\n",
    "#!/bin/bash\n",
    "\n",
    "docker compose down --volumes --rmi all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0512a5-f084-4e89-8439-cae049126dab",
   "metadata": {},
   "source": [
    "### Test dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601d378f-0301-4949-89c6-3266ed9a2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/meng/work/Airflow/dags/test_dag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/work/Airflow/dags/test_dag.py\n",
    "import datetime\n",
    "import pendulum\n",
    "from airflow.models.dag import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "now = pendulum.now(tz=\"UTC\")\n",
    "now_to_the_hour = (now - datetime.timedelta(0, 0, 0, 0, 0, 3))\\\n",
    "    .replace(minute=0, second=0, microsecond=0)\n",
    "START_DATE = now_to_the_hour\n",
    "DAG_NAME = \"test_dag_v1\"\n",
    "\n",
    "dag = DAG(\n",
    "    DAG_NAME,\n",
    "    schedule=\"*/10 * * * *\",\n",
    "    default_args={\"depends_on_past\": True},\n",
    "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "run_this_1 = EmptyOperator(task_id=\"run_this_1\", dag=dag)\n",
    "run_this_2 = EmptyOperator(task_id=\"run_this_2\", dag=dag)\n",
    "run_this_2.set_upstream(run_this_1)\n",
    "run_this_3 = EmptyOperator(task_id=\"run_this_3\", dag=dag)\n",
    "run_this_3.set_upstream(run_this_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
